#!/usr/bin/env python3
"""
æœºå™¨å­¦ä¹ é€‰è‚¡æ¨¡å—ï¼ˆå¼€æºç‰ˆï¼‰

ä½¿ç”¨æœºå™¨å­¦ä¹ è¿›è¡Œè‚¡ç¥¨é¢„æµ‹
åŸºäº scikit-learnï¼Œæ— éœ€ä»˜è´¹ä¾èµ–
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

try:
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("è­¦å‘Š: scikit-learn ä¸å¯ç”¨")


@dataclass
class MLPredictionResult:
    """MLé¢„æµ‹ç»“æœ"""
    symbol: str            # è‚¡ç¥¨ä»£ç 
    signal: str           # é¢„æµ‹ä¿¡å· (UP/DOWN)
    up_probability: float  # ä¸Šæ¶¨æ¦‚ç‡
    down_probability: float  # ä¸‹è·Œæ¦‚ç‡
    confidence: float     # ç½®ä¿¡åº¦
    accuracy: float       # æ¨¡å‹å‡†ç¡®ç‡
    feature_importance: Dict  # ç‰¹å¾é‡è¦æ€§


class MLSelector:
    """æœºå™¨å­¦ä¹ é€‰è‚¡å™¨ï¼ˆå¼€æºç‰ˆï¼‰"""
    
    def __init__(
        self,
        model_type: str = 'random_forest',
        n_estimators: int = 100,
        max_depth: int = 5,
        test_size: float = 0.2
    ):
        """
        åˆå§‹åŒ–MLé€‰è‚¡å™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ('random_forest', 'gradient_boosting', 'logistic')
            n_estimators: æ ‘çš„æ•°é‡
            max_depth: æœ€å¤§æ·±åº¦
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
        """
        self.model_type = model_type
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.test_size = test_size
        
        self.model = None
        self.scaler = None
        self.feature_names = []
        self.accuracy = 0
        
    def _create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        if self.model_type == 'random_forest':
            return RandomForestClassifier(
                n_estimators=self.n_estimators,
                max_depth=self.max_depth,
                random_state=42
            )
        elif self.model_type == 'gradient_boosting':
            return GradientBoostingClassifier(
                n_estimators=self.n_estimators,
                max_depth=self.max_depth,
                random_state=42
            )
        else:  # logistic
            return LogisticRegression(
                max_iter=1000,
                random_state=42
            )
    
    def _prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        
        Args:
            df: è‚¡ç¥¨æ•°æ®
        
        Returns:
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾ (1=ä¸Šæ¶¨, 0=ä¸‹è·Œ)
        """
        # è®¡ç®—ç‰¹å¾
        df = df.copy()
        
        # åŸºç¡€ç‰¹å¾
        df['ma5'] = df['close'].rolling(5).mean()
        df['ma10'] = df['close'].rolling(10).mean()
        df['ma20'] = df['close'].rolling(20).mean()
        
        # ä»·æ ¼å˜åŒ–ç‡
        df['price_change'] = df['close'].pct_change()
        df['price_change_5'] = df['close'].pct_change(5)
        
        # æˆäº¤é‡å˜åŒ–
        df['volume_change'] = df['volume'].pct_change()
        df['volume_ma5'] = df['volume'].rolling(5).mean()
        
        # RSI
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0)
        loss = (-delta).where(delta < 0, 0)
        avg_gain = gain.rolling(14).mean()
        avg_loss = loss.rolling(14).mean()
        rs = avg_gain / (avg_loss + 0.0001)
        df['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        
        # æ³¢åŠ¨ç‡
        df['volatility'] = df['close'].rolling(20).std() / df['close'].rolling(20).mean()
        
        # åŠ¨é‡
        df['momentum_5'] = df['close'] / df['close'].shift(5) - 1
        df['momentum_10'] = df['close'] / df['close'].shift(10) - 1
        
        # åˆ›å»ºæ ‡ç­¾ï¼šæœªæ¥5å¤©æ¶¨è·Œ
        df['future_return'] = df['close'].shift(-5) / df['close'] - 1
        df['label'] = (df['future_return'] > 0).astype(int)
        
        # ç‰¹å¾åˆ—è¡¨
        feature_cols = [
            'ma5', 'ma10', 'ma20',  # ç§»åŠ¨å¹³å‡
            'price_change', 'price_change_5',  # ä»·æ ¼å˜åŒ–
            'volume_change', 'volume_ma5',  # æˆäº¤é‡
            'rsi',  # RSI
            'macd',  # MACD
            'volatility',  # æ³¢åŠ¨ç‡
            'momentum_5', 'momentum_10',  # åŠ¨é‡
        ]
        
        self.feature_names = feature_cols
        
        # åˆ é™¤NaN
        df = df.dropna(subset=feature_cols + ['label'])
        
        if len(df) < 50:
            return None, None
        
        X = df[feature_cols].values
        y = df['label'].values
        
        return X, y
    
    def train(self, df: pd.DataFrame, verbose: bool = True) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            df: è‚¡ç¥¨æ•°æ®
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            Dict: è®­ç»ƒç»“æœ
        """
        if not SKLEARN_AVAILABLE:
            return {'success': False, 'error': 'scikit-learn ä¸å¯ç”¨'}
        
        try:
            # å‡†å¤‡æ•°æ®
            X, y = self._prepare_features(df)
            
            if X is None:
                return {'success': False, 'error': 'æ•°æ®ä¸è¶³'}
            
            # æ ‡å‡†åŒ–
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X)
            
            # åˆ†å‰²æ•°æ®
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, 
                test_size=self.test_size, 
                random_state=42
            )
            
            # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
            self.model = self._create_model()
            self.model.fit(X_train, y_train)
            
            # è¯„ä¼°
            y_pred = self.model.predict(X_test)
            self.accuracy = accuracy_score(y_test, y_pred)
            
            if verbose:
                print(f"\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
                print(f"   æ¨¡å‹ç±»å‹: {self.model_type}")
                print(f"   å‡†ç¡®ç‡: {self.accuracy:.2%}")
                print(f"   è®­ç»ƒæ ·æœ¬: {len(X_train)}")
                print(f"   æµ‹è¯•æ ·æœ¬: {len(X_test)}")
            
            # äº¤å‰éªŒè¯
            if len(X_scaled) >= 50:
                cv_scores = cross_val_score(self.model, X_scaled, y, cv=5)
                cv_accuracy = cv_scores.mean()
            else:
                cv_accuracy = self.accuracy
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            if hasattr(self.model, 'feature_importances_'):
                importance = dict(zip(self.feature_names, self.model.feature_importances_))
            elif hasattr(self.model, 'coef_'):
                importance = dict(zip(self.feature_names, np.abs(self.model.coef_[0])))
            else:
                importance = {}
            
            return {
                'success': True,
                'accuracy': self.accuracy,
                'cv_accuracy': cv_accuracy,
                'feature_importance': importance,
                'train_samples': len(X_train),
                'test_samples': len(X_test)
            }
            
        except Exception as e:
            if verbose:
                print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def predict(self, df: pd.DataFrame) -> MLPredictionResult:
        """
        é¢„æµ‹
        
        Args:
            df: è‚¡ç¥¨æ•°æ®
        
        Returns:
            MLPredictionResult: é¢„æµ‹ç»“æœ
        """
        if not SKLEARN_AVAILABLE or self.model is None:
            # è¿”å›é»˜è®¤ç»“æœ
            return MLPredictionResult(
                symbol="",
                signal="HOLD",
                up_probability=0.5,
                down_probability=0.5,
                confidence=0.5,
                accuracy=0.5,
                feature_importance={}
            )
        
        try:
            # å‡†å¤‡æœ€æ–°æ•°æ®
            df = df.copy()
            
            # è®¡ç®—ç‰¹å¾
            df['ma5'] = df['close'].rolling(5).mean()
            df['ma10'] = df['close'].rolling(10).mean()
            df['ma20'] = df['close'].rolling(20).mean()
            df['price_change'] = df['close'].pct_change()
            df['price_change_5'] = df['close'].pct_change(5)
            df['volume_change'] = df['volume'].pct_change()
            df['volume_ma5'] = df['volume'].rolling(5).mean()
            
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0)
            loss = (-delta).where(delta < 0, 0)
            avg_gain = gain.rolling(14).mean()
            avg_loss = loss.rolling(14).mean()
            rs = avg_gain / (avg_loss + 0.0001)
            df['rsi'] = 100 - (100 / (1 + rs))
            
            ema12 = df['close'].ewm(span=12).mean()
            ema26 = df['close'].ewm(span=26).mean()
            df['macd'] = ema12 - ema26
            
            df['volatility'] = df['close'].rolling(20).std() / df['close'].rolling(20).mean()
            df['momentum_5'] = df['close'] / df['close'].shift(5) - 1
            df['momentum_10'] = df['close'] / df['close'].shift(10) - 1
            
            # è·å–æœ€æ–°æ•°æ®
            latest = df.iloc[-1]
            
            feature_cols = [
                'ma5', 'ma10', 'ma20',
                'price_change', 'price_change_5',
                'volume_change', 'volume_ma5',
                'rsi', 'macd', 'volatility',
                'momentum_5', 'momentum_10',
            ]
            
            X = latest[feature_cols].values.reshape(1, -1)
            X_scaled = self.scaler.transform(X)
            
            # é¢„æµ‹
            prediction = self.model.predict(X_scaled)[0]
            probabilities = self.model.predict_proba(X_scaled)[0]
            
            # ç¡®å®šä¿¡å·
            up_prob = probabilities[1] if len(probabilities) > 1 else 0.5
            down_prob = probabilities[0] if len(probabilities) > 0 else 0.5
            
            if up_prob > 0.55:
                signal = "UP"
                confidence = up_prob
            elif down_prob > 0.55:
                signal = "DOWN"
                confidence = down_prob
            else:
                signal = "HOLD"
                confidence = 0.5 + abs(up_prob - down_prob)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            if hasattr(self.model, 'feature_importances_'):
                importance = dict(zip(self.feature_names, self.model.feature_importances_))
            else:
                importance = {}
            
            return MLPredictionResult(
                symbol="",
                signal=signal,
                up_probability=up_prob,
                down_probability=down_prob,
                confidence=confidence,
                accuracy=self.accuracy,
                feature_importance=importance
            )
            
        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥: {e}")
            return MLPredictionResult(
                symbol="",
                signal="HOLD",
                up_probability=0.5,
                down_probability=0.5,
                confidence=0.5,
                accuracy=self.accuracy,
                feature_importance={}
            )
    
    def get_feature_importance(self) -> pd.DataFrame:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if not hasattr(self.model, 'feature_importances_'):
            return pd.DataFrame()
        
        importance_df = pd.DataFrame({
            'ç‰¹å¾': self.feature_names,
            'é‡è¦æ€§': self.model.feature_importances_
        }).sort_values('é‡è¦æ€§', ascending=False)
        
        return importance_df


# ä¾¿æ·å‡½æ•°
def quick_ml_predict(df: pd.DataFrame) -> MLPredictionResult:
    """å¿«é€ŸMLé¢„æµ‹"""
    selector = MLSelector(model_type='random_forest')
    result = selector.train(df, verbose=False)
    
    if result.get('success'):
        return selector.predict(df)
    else:
        return MLPredictionResult(
            symbol="",
            signal="HOLD",
            up_probability=0.5,
            down_probability=0.5,
            confidence=0.5,
            accuracy=0.5,
            feature_importance={}
        )


if __name__ == "__main__":
    print("MLé€‰è‚¡æ¨¡å—æµ‹è¯•")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    dates = pd.date_range(start="2024-01-01", periods=500, freq="D")
    np.random.seed(42)
    
    # ç”Ÿæˆå¸¦è¶‹åŠ¿çš„ä»·æ ¼æ•°æ®
    prices = 100 + np.cumsum(np.random.randn(500) * 0.5 + 0.05)
    
    df = pd.DataFrame({
        'date': dates,
        'close': prices,
        'open': prices * (1 + np.random.randn(500) * 0.01),
        'high': prices * (1 + np.abs(np.random.randn(500) * 0.02)),
        'low': prices * (1 - np.abs(np.random.randn(500) * 0.02)),
        'volume': np.random.randint(1000000, 10000000, 500),
    })
    
    # åˆ›å»ºé€‰è‚¡å™¨
    selector = MLSelector(model_type='random_forest')
    
    # è®­ç»ƒ
    print("\nè®­ç»ƒæ¨¡å‹...")
    result = selector.train(df, verbose=True)
    
    if result['success']:
        # é¢„æµ‹
        print("\né¢„æµ‹...")
        pred = selector.predict(df)
        
        print(f"\nğŸ“Š é¢„æµ‹ç»“æœ:")
        print(f"   ä¿¡å·: {pred.signal}")
        print(f"   ä¸Šæ¶¨æ¦‚ç‡: {pred.up_probability:.1%}")
        print(f"   ä¸‹è·Œæ¦‚ç‡: {pred.down_probability:.1%}")
        print(f"   ç½®ä¿¡åº¦: {pred.confidence:.1%}")
        print(f"   æ¨¡å‹å‡†ç¡®ç‡: {pred.accuracy:.1%}")
        
        # ç‰¹å¾é‡è¦æ€§
        print(f"\nğŸ“ˆ ç‰¹å¾é‡è¦æ€§ (Top 5):")
        importance = pred.feature_importance
        if importance:
            sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]
            for feat, imp in sorted_importance:
                print(f"   {feat}: {imp:.3f}")
    else:
        print(f"è®­ç»ƒå¤±è´¥: {result.get('error')}")
